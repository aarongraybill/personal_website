{
  "hash": "cedfc5a201bd710c63721caad94d0f82",
  "result": {
    "markdown": "---\ntitle: \"Uncovering the Community Structure of Wikipedia's Links\"\ndescription: \"In this post, \"\nauthor: \"Aaron Graybill\"\ndate: \"10/23/2023\"\ndraft: true\nfreeze: true\n---\n\n\n## Introduction\n\nAs an avid Wikipedia user, you will surely know that Wikipedia contains links from your current article to other articles, so that you can gain additional information about related topics. As seen below, we link from the [Grace Hopper article](https://en.wikipedia.org/wiki/Grace_Hopper) to the [Eckert--Mauchly Computer Corporation article](https://en.wikipedia.org/wiki/Eckert%E2%80%93Mauchly_Computer_Corporation)\n\n\n```{=html}\n<iframe src=\"https://en.wikipedia.org/wiki/Grace_Hopper#UNIVAC\" \nheight=\"400\" \nwidth=\"90%\" \ntitle=\"Example Wikipedia Article\"\nid=frame\n>\n</iframe>\n```\n\nThese links are useful in themselves, but they also provide insight into the way that we as humans categorize and connect information. In this article, I will will look at two categories of Wikipedia pages, Music and Mathematics. But before we get ahead of ourselves, we need some data.\n\n## The Data\n\n### How does Wikipedia Store Links between Pages\n\nYou might anticipate that whenever you click on a page link, the authors of the article included a url to the page being referenced. However, wikipedia does not operate that way. Under the hood, when a Wikipedia editor wants to link to another page, they reference the page title in double square brackets. For example, to point to the page on Bob Dylan, you type `[[Bob_Dylan]]` into the editor. Wikipedia grabs the correct url on the backend. You might be worried that this would cause name collisions, and it can! However, article titles are essentially unique page identifiers. For example, Anne Hathaway, the modern day actress has url: <https://en.wikipedia.org/wiki/Anne_Hathaway> whereas Anne Hathaway, wife of William Shakespeare, is given url: <https://en.wikipedia.org/wiki/Anne_Hathaway_(wife_of_Shakespeare)>.\n\nYou can see that these article titles implicitly disambiguate all of the Anne Hathaways.\n\n### Getting the information on page links\n\nWikipedia goes above and beyond with its *free* API, so we can easily grab links between pages. To keep our code simple, I'll use the excellent [Wikipedia-API](https://github.com/martin-majlis/Wikipedia-API) package. However, I have worked with the bare API in the past, and it is relatively easy to use, so it should be feasible to replicate this code in your language of choice.\n\nHowever, despite Wikipedia making it easy to get page links, we have to be selective about which pages we are willing to consider. At time of writing, the English Wikipedia already has well more than six million articles. So to limit the pages we consider, let's only consider artiles falling into certain \"categories.\"  The code given below provides a method to walk through all subcategories of a given category and collect all pages. I use a `max_depth` parameter to ensure we terminate within a reasonable amount of time. Here, I make extensive use of python's `set` class which makes it easy to perform operations on collections that don't have a meaningful order and each item cannot be duplicated. For example, I use the `union` function to add two sets of pages together without worrying about duplicating pages.\n\n\n::: {.cell}\n\n```{.python .cell-code}\nimport wikipediaapi\n\nwiki_wiki = wikipediaapi.Wikipedia('Link Network (my_hidden_email@email_provider.com)', 'simple')\n\ndef get_category_members(category_name: str,max_depth=5):\n    iter_num = 0\n    explored_cats = set()\n    pages = set()\n    # Initialize the unexplored category to the initial category\n    # give the initial category depth zero\n    unexplored_cats = {wiki_wiki.page(category_name):0}\n    \n    # While there are unexplored categories continue to loop\n    while len(unexplored_cats)>0:\n        # print where we're at\n        print_str = f\"Iteration Number: {iter_num}, Unexplored Categories: {len(unexplored_cats)}\"\n        print(print_str)\n        \n        # get an arbitrary unexplored category (removing it from the set)\n        working_cat, depth = unexplored_cats.popitem()\n        \n        # If this particular category is not too deep, get its information\n        if depth <= max_depth:\n            \n            # Add the current category to list of explored\n            explored_cats = explored_cats.union({working_cat})\n            \n            # Categories have namespace 14, so only look at those\n            new_cats = {v for k,v in working_cat.categorymembers.items() if v.ns==14}\n            new_cats = new_cats.difference(explored_cats) #ignore already discovered categories\n            new_cats = {x:depth+1 for x in new_cats} # add depth to each category\n            unexplored_cats = unexplored_cats | new_cats # add new categories to unexploored\n\n            # pages have namespace zero, add the new categories to the lisits\n            new_pages = {k for k,v in working_cat.categorymembers.items() if v.ns==0}\n            pages = pages.union(new_pages)\n\n    return pages\n```\n:::\n\n\nOnce we know which pages fall into which categories, we have a reasonably self-contanied sample of pages on which to fetch the pagelinks. The followinig function accepts a set of pages, and creates a csv of all of the links between those pages. Here, I only consider links that remain within the initial pages, so pages outside of the category are ignored.\n\n\n::: {.cell}\n\n```{.python .cell-code}\nimport csv\ndef create_edge_list(pages: set[str],filename: str):\n    with open(filename,'w') as csvout:\n        writer = csv.writer(csvout, lineterminator='\\n')\n        writer.writerow((\"source\",\"target\"))\n        unexplored = pages\n        while len(unexplored)>0:\n            print_str = f\"Left to explore: {len(unexplored)}\"\n            print(print_str)\n            page = unexplored.pop()\n            out_links = {k for k,v in wiki_wiki.page(page).links.items() if v.ns==0}\n            out_links = out_links.intersection(pages)\n            new_edges = [(page,v) for v in out_links]\n            writer.writerows(new_edges)\n        return None\n```\n:::\n\n\nThe only bit of pizzaz going on here is my use of the the `csv.writer` object which allows me to iteratively append new rows to a csv without having to store all of the edges in memory. Essentially, python adds an edge to the bottom of the csv, and then forgets that edge exists. Since this network is directed (linking to a page is not the same thing as a page linking to you), we can iterate through the pages without worrying about this process producing duplicates.\n\nI collect this page link data for the [Mathematics Category](https://en.wikipedia.org/wiki/Category:Mathematics) and the [Music Category](https://en.wikipedia.org/wiki/Category:Music). I consider all pages that are two or fewer subcategories deep. So pages that are in subcategories of subcategories of subcategories are excluded.\n\n## Exploratory Analysis\n\nBefore we get into the more network analysis, we should quickly get acquainted with the data in its rawest form.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(dplyr)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n\nAttaching package: 'dplyr'\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n```\n:::\n\n```{.r .cell-code}\nmusic_df <- data.table::fread('~/projects/wikipedia/music_links.csv')\nmath_df <- data.table::fread('~/projects/wikipedia/math_links.csv')\n\nknitr::kable(\n  data.frame(\n    Category = c(\"Music\", \"Mathematics\"),\n    Pages = c(n_distinct(c(music_df$source,music_df$target)),n_distinct(c(math_df$source,math_df$target))),\n    Links = c(nrow(music_df),nrow(math_df))\n  )\n)\n```\n\n::: {.cell-output-display}\n|Category    | Pages|  Links|\n|:-----------|-----:|------:|\n|Music       | 12821| 107939|\n|Mathematics | 10149|  94779|\n:::\n:::\n\n\nSo we can see that even when you only look at subcategories of subcategories, there's already a very large number of pages to consider. Let's say we wanted to know which pages within music are related to one another. We could pick one page and look at which pages it connects too, but that doesn't scale well. Clearly, we need some computational techniques to identify the communities within this network.\n\nCommunity detection is a notoriously difficult task in network science, so we should be smart about the method we choose. For the sake of this example, I'm using the walktrap algorithm, published [here](https://arxiv.org/abs/1403.2805). This method is based on the following idea:\n\n1. Start on a random page, and then randomly pick a page that your current page links to.[^1]\n2. Then on that next page, pick a new random page\n3. Continue this process a total of $t$ times.\n\n[^1]: In our case, we only consider links to pages that fall in the same category as the starting page.\n\nThis procedure creates a probability distribution over which pages connect to which pages. However, looking at this probability distribution by itself would have a major problem. Important or central pages (in graph theory lingo, nodes of high degree) would end up in every community. This doesn't make for meaningful communities, so the algorithm compensates for the number of incoming and outgoing links that a page has. The algorithm then implements a clever way of using this compensated probability distribution to group together pages that are similar.\n\nHowever, we're not completely out of the woods. We need to decide an appropriate value of $t$, the number of links to click on our random walk.\n\n\n```{ojs}\n//| echo: false\ndata = FileAttachment(\"example_walktrap.json\").json()\n\n```\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(igraph)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n\nAttaching package: 'igraph'\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following objects are masked from 'package:dplyr':\n\n    as_data_frame, groups, union\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following objects are masked from 'package:stats':\n\n    decompose, spectrum\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following object is masked from 'package:base':\n\n    union\n```\n:::\n\n```{.r .cell-code}\nset.seed(314)\ng <- sample_islands(5,10,.1,3)\nclust <- cluster_walktrap(g,steps = 1)\n\nedge.weights <- function(community, network, weight.within = 100, weight.between = 1) {\n# Stolen from here: https://stackoverflow.com/a/31976350/19400318\nbridges <- crossing(communities = community, graph = network)\nweights <- ifelse(test = bridges, yes = weight.between, no = weight.within)\nreturn(weights) \n}\n\nV(g)$membership <- clust$membership\nE(g)$weight <- edge.weights(clust,g)\n\n# convert to json with directional edges   \nl <- d3r::d3_igraph(g,json = F)\n#l <- within(l)\njsonlite::write_json(l,\"example_walktrap.json\")\nx <- jsonlite::toJSON (l, pretty = TRUE)\ncon <- file (\"example_walktrap.json\")\nwriteLines (x, con)\nclose (con)\n```\n:::\n\n```{ojs}\nviewof t_value = Inputs.range(\n  [1,10], \n  {value: 4, step: 1, label: \"Number of Steps (t):\"}\n)\n```\n\n```{ojs}\n//| echo: false\nchart = {\n  // Specify the dimensions of the chart.\n  const width = 928;\n  const height = 600;\n\n  // Specify the color scale.\n  const color = d3.scaleOrdinal(d3.schemeCategory10);\n\n  // The force simulation mutates links and nodes, so create a copy\n  // so that re-evaluating this cell produces the same result.\n  const links = data.links.map(d => ({...d}));\n  const nodes = data.nodes.map(d => ({...d}));\n\n  // Create a simulation with several forces.\n  const simulation = d3.forceSimulation(nodes)\n      .force(\"link\", d3.forceLink(links).id(d => d.id))\n      .force(\"charge\", \n              d3.forceManyBody()\n              //.strength(d => d.weight*(d.weight-1))\n      )\n      .force(\"center\", d3.forceCenter(width / 2, height / 2))\n      .on(\"tick\", ticked);\n\n  // Create the SVG container.\n  const svg = d3.create(\"svg\")\n      .attr(\"width\", width)\n      .attr(\"height\", height)\n      .attr(\"viewBox\", [0, 0, width, height])\n      .attr(\"style\", \"max-width: 100%; height: auto;\");\n\n  // Add a line for each link, and a circle for each node.\n  const link = svg.append(\"g\")\n      .attr(\"stroke-opacity\", 0.6)\n    .selectAll()\n    .data(links)\n    .join(\"line\")\n    .attr(\"stroke-width\", d => Math.sqrt(d.value))\n    .attr(\"stroke\", d => d.weight);\n\n  const node = svg.append(\"g\")\n      .attr(\"stroke\", \"#fff\")\n      .attr(\"stroke-width\", 1.5)\n    .selectAll()\n    .data(nodes)\n    .join(\"circle\")\n      .attr(\"r\", 2*t_value)\n      .attr(\"fill\", d => color(d.membership));\n\n  node.append(\"title\")\n      .text(d => d.id);\n\n  // Add a drag behavior.\n  node.call(d3.drag()\n        .on(\"start\", dragstarted)\n        .on(\"drag\", dragged)\n        .on(\"end\", dragended));\n\n  // Set the position attributes of links and nodes each time the simulation ticks.\n  function ticked() {\n    link\n        .attr(\"x1\", d => d.source.x)\n        .attr(\"y1\", d => d.source.y)\n        .attr(\"x2\", d => d.target.x)\n        .attr(\"y2\", d => d.target.y);\n\n    node\n        .attr(\"cx\", d => d.x)\n        .attr(\"cy\", d => d.y);\n  }\n\n  // Reheat the simulation when drag starts, and fix the subject position.\n  function dragstarted(event) {\n    if (!event.active) simulation.alphaTarget(0.3).restart();\n    event.subject.fx = event.subject.x;\n    event.subject.fy = event.subject.y;\n  }\n\n  // Update the subject (dragged node) position during drag.\n  function dragged(event) {\n    event.subject.fx = event.x;\n    event.subject.fy = event.y;\n  }\n\n  // Restore the target alpha so the simulation cools after dragging ends.\n  // Unfix the subject position now that it’s no longer being dragged.\n  function dragended(event) {\n    if (!event.active) simulation.alphaTarget(0);\n    event.subject.fx = null;\n    event.subject.fy = null;\n  }\n\n  // When this cell is re-run, stop the previous simulation. (This doesn’t\n  // really matter since the target alpha is zero and the simulation will\n  // stop naturally, but it’s a good practice.)\n  invalidation.then(() => simulation.stop());\n\n  return svg.node();\n}\n```\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}